{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import sqrt\n",
        "from scipy import stats\n",
        "from torch_geometric.data import InMemoryDataset, DataLoader\n",
        "from torch_geometric import data as DATA\n",
        "from lifelines.utils import concordance_index\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "import json,pickle\n",
        "from collections import OrderedDict\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import MolFromSmiles\n",
        "import networkx as nx\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Sequential, Linear, ReLU\n",
        "from torch_geometric.nn import GINConv, global_add_pool\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "\n",
        "import sys, os\n",
        "from random import shuffle"
      ],
      "metadata": {
        "id": "4N__c00_5rxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much of this code is from/is adapted from https://github.com/thinng/GraphDTA\n",
        "There paper is cited as\n",
        "Nguyen, T., Le, H., Quinn, T. P., Nguyen, T., Le, T. D., & Venkatesh, S. (2021). GraphDTA:\n",
        "Predicting drugâ€“target binding affinity with graph neural networks. Bioinformatics, 37(8), 1140-1147."
      ],
      "metadata": {
        "id": "mfnY7gxq67IX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lqh5cyYD-9DL"
      },
      "outputs": [],
      "source": [
        "class TestbedDataset(InMemoryDataset):\n",
        "    def __init__(self, root='/tmpns', dataset='',\n",
        "                 xd=None, y=None, transform=None,\n",
        "                 pre_transform=None,smile_graph=None, tu = False):\n",
        "\n",
        "        #root is required for save preprocessed data, default is '/tmp'\n",
        "        super(TestbedDataset, self).__init__(root, transform, pre_transform)\n",
        "        self.dataset = dataset\n",
        "        if (os.path.isfile(self.processed_paths[0]) and tu):\n",
        "            print('Pre-processed data found: {}, loading ...'.format(self.processed_paths[0]))\n",
        "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "        else:\n",
        "            print('Pre-processed data {} not found, doing pre-processing...'.format(self.processed_paths[0]))\n",
        "            self.process(xd, y,smile_graph)\n",
        "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return [self.dataset + '.pt']\n",
        "\n",
        "    def _process(self):\n",
        "        if not os.path.exists(self.processed_dir):\n",
        "            os.makedirs(self.processed_dir)\n",
        "\n",
        "\n",
        "    def process(self, xd, y,smile_graph):\n",
        "        assert (len(xd) == len(y)), \"The three lists must be the same length!\"\n",
        "        data_list = []\n",
        "        data_len = len(xd)\n",
        "        for i in range(data_len):\n",
        "            print('Converting SMILES to graph: {}/{}'.format(i+1, data_len))\n",
        "            smiles = xd[i]\n",
        "            labels = y[i]\n",
        "            # convert SMILES to molecular representation using rdkit\n",
        "            c_size, features, edge_index = smile_graph[smiles]\n",
        "            # make the graph ready for PyTorch Geometrics GCN algorithms:\n",
        "            GCNData = DATA.Data(x=torch.Tensor(features),\n",
        "                                edge_index=torch.LongTensor(edge_index).transpose(1, 0),\n",
        "                                y=torch.FloatTensor([labels]))\n",
        "            GCNData.__setitem__('c_size', torch.LongTensor([c_size]))\n",
        "            # append graph, label and target sequence to data list\n",
        "            data_list.append(GCNData)\n",
        "\n",
        "        if self.pre_filter is not None:\n",
        "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            data_list = [self.pre_transform(data) for data in data_list]\n",
        "        print('Graph construction done. Saving to file.')\n",
        "        data, slices = self.collate(data_list)\n",
        "        # save preprocessed data:\n",
        "        torch.save((data, slices), self.processed_paths[0])\n",
        "\n",
        "def rmse(y,f):\n",
        "    rmse = sqrt(((y - f)**2).mean(axis=0))\n",
        "    return rmse\n",
        "def mse(y,f):\n",
        "    mse = ((y - f)**2).mean(axis=0)\n",
        "    return mse\n",
        "\n",
        "def mae(y,f):\n",
        "    return mean_absolute_error(y,f)\n",
        "\n",
        "def pearson(y,f):\n",
        "    rp = np.corrcoef(y, f)[0,1]\n",
        "    return rp\n",
        "def spearman(y,f):\n",
        "    rs = stats.spearmanr(y, f)[0]\n",
        "    return rs\n",
        "\n",
        "def ci(y,f):\n",
        "    return concordance_index(y,f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eim0e6saCQu8"
      },
      "outputs": [],
      "source": [
        "def atom_features(atom):\n",
        "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na','Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb','Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H','Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr','Cr', 'Pt', 'Hg', 'Pb', 'Unknown']) +\n",
        "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
        "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
        "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
        "                    [atom.GetIsAromatic()])\n",
        "\n",
        "def one_of_k_encoding(x, allowable_set):\n",
        "    if x not in allowable_set:\n",
        "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "def one_of_k_encoding_unk(x, allowable_set):\n",
        "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
        "    if x not in allowable_set:\n",
        "        x = allowable_set[-1]\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "def smile_to_graph(smile):\n",
        "    mol = Chem.MolFromSmiles(smile)\n",
        "\n",
        "    c_size = mol.GetNumAtoms()\n",
        "\n",
        "    features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        feature = atom_features(atom)\n",
        "        features.append( feature / sum(feature) )\n",
        "\n",
        "    edges = []\n",
        "    for bond in mol.GetBonds():\n",
        "        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
        "    g = nx.Graph(edges).to_directed()\n",
        "    edge_index = []\n",
        "    for e1, e2 in g.edges:\n",
        "        edge_index.append([e1, e2])\n",
        "\n",
        "    return c_size, features, edge_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5bRqpv-5ZJQJ"
      },
      "outputs": [],
      "source": [
        "with open(\"smile_graph.pkl\", 'rb') as handle:\n",
        "  smile_graph = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "G-Id_VUBpev8"
      },
      "outputs": [],
      "source": [
        "class GINConvNet(torch.nn.Module):\n",
        "    def __init__(self, n_output=1,num_features_xd=78, num_features_xt=25,\n",
        "                 n_filters=32, embed_dim=128, output_dim=128, dropout=0.2):\n",
        "\n",
        "        super(GINConvNet, self).__init__()\n",
        "\n",
        "        dim = 32\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.n_output = n_output\n",
        "\n",
        "        nn1 = Sequential(Linear(num_features_xd, dim), ReLU(), Linear(dim, dim))\n",
        "        self.conv1 = GINConv(nn1)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(dim)\n",
        "\n",
        "        nn2 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
        "        self.conv2 = GINConv(nn2)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(dim)\n",
        "\n",
        "        nn3 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
        "        self.conv3 = GINConv(nn3)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(dim)\n",
        "\n",
        "        nn4 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
        "        self.conv4 = GINConv(nn4)\n",
        "        self.bn4 = torch.nn.BatchNorm1d(dim)\n",
        "\n",
        "        nn5 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
        "        self.conv5 = GINConv(nn5)\n",
        "        self.bn5 = torch.nn.BatchNorm1d(dim)\n",
        "\n",
        "        self.fc1_xd = Linear(dim, int((dim*2)/3))\n",
        "\n",
        "\n",
        "        self.fc2_xd = Linear(int((dim*2)/3), int((dim*4)/9))\n",
        "\n",
        "        self.out = nn.Linear(int((dim*4)/9), self.n_output)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(self.conv4(x, edge_index))\n",
        "        x = self.bn4(x)\n",
        "        x = F.relu(self.conv5(x, edge_index))\n",
        "        x = self.bn5(x)\n",
        "        x = global_add_pool(x, batch)\n",
        "        x = F.relu(self.fc2_xd(F.relu(self.fc1_xd(x))))\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "\n",
        "        out = self.out(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zxqHM9tgS0Lb"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    print('Training on {} samples...'.format(len(train_loader.dataset)))\n",
        "    model.train()\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_fn(output, data.y.view(-1, 1).float().to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % LOG_INTERVAL == 0:\n",
        "            print('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n",
        "                                                                           batch_idx * len(data.x),\n",
        "                                                                           len(train_loader.dataset),\n",
        "                                                                           100. * batch_idx / len(train_loader),\n",
        "                                                                           loss.item()))\n",
        "\n",
        "def predicting(model, device, loader):\n",
        "    model.eval()\n",
        "    total_preds = torch.Tensor()\n",
        "    total_labels = torch.Tensor()\n",
        "    print('Make prediction for {} samples...'.format(len(loader.dataset)))\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "            total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
        "            total_labels = torch.cat((total_labels, data.y.view(-1, 1).cpu()), 0)\n",
        "    return total_labels.numpy().flatten(),total_preds.numpy().flatten()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "W7VuVTRuLJG3"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('test.csv')\n",
        "test_drugs, test_Y = list(df['SMILES']),list(df['3CLPro_pocket1'])\n",
        "test_drugs, test_Y = np.asarray(test_drugs), np.asarray(test_Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXoddmY1SDFp"
      },
      "outputs": [],
      "source": [
        "targs = ['3CLPro_pocket1', 'ADRP-ADPR_pocket1', 'ADRP-ADPR_pocket5',\n",
        "       'ADRP_pocket1', 'ADRP_pocket12', 'ADRP_pocket13', 'COV_pocket1',\n",
        "       'COV_pocket2', 'COV_pocket8', 'COV_pocket10', 'NSP9_pocket2',\n",
        "       'NSP9_pocket7', 'NSP15_pocket1', 'ORF7A_pocket2',\n",
        "       'PLPro_chainA_pocket3', 'PLPro_chainA_pocket23', 'PLPro_pocket6',\n",
        "       'PLPro_pocket50']\n",
        "for i in targs:\n",
        "  df = pd.read_csv('train.csv')\n",
        "  train_drugs, train_Y = list(df['SMILES']),list(df[i])\n",
        "  train_drugs, train_Y = np.asarray(train_drugs), np.asarray(train_Y)\n",
        "  print('test: ')\n",
        "  test_data = TestbedDataset(dataset='test.csv', xd=test_drugs, y=test_Y,smile_graph=smile_graph, tu = True)\n",
        "  print('train: ')\n",
        "  train_data = TestbedDataset(dataset='train.csv', xd=train_drugs, y=train_Y,smile_graph=smile_graph, tu = False)\n",
        "\n",
        "  TRAIN_BATCH_SIZE = 512\n",
        "  TEST_BATCH_SIZE = 512\n",
        "  LR = 0.0005\n",
        "  LOG_INTERVAL = 20\n",
        "  NUM_EPOCHS = 1000\n",
        "\n",
        "  train_size = int(0.8 * len(train_data))\n",
        "  valid_size = len(train_data) - train_size\n",
        "  train_data, valid_data = torch.utils.data.random_split(train_data, [train_size, valid_size])\n",
        "\n",
        "  train_loader = DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "  valid_loader = DataLoader(valid_data, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
        "  test_loader = DataLoader(test_data, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
        "\n",
        "  device = torch.device(\"cuda:{}\".format(0)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "  model = GINConvNet().to(device)\n",
        "  loss_fn = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "\n",
        "  saved_p = {}\n",
        "  mse_d = {}\n",
        "  mae_d = {}\n",
        "  ci_d = {}\n",
        "  best_mse = 1000\n",
        "  best_mae = 1000\n",
        "  best_ci = 0\n",
        "  best_epoch = -1\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    train(model, device, train_loader, optimizer, epoch+1)\n",
        "    print('predicting for valid data')\n",
        "    G,P = predicting(model, device, valid_loader)\n",
        "    ret = [mse(G,P),mae(G,P),ci(G,P)]\n",
        "    mse_d[epoch] = {ret[0]}\n",
        "    mae_d[epoch] = {ret[1]}\n",
        "    ci_d[epoch] = {ret[2]}\n",
        "    val = ret[1]\n",
        "    if val<best_mae:\n",
        "      best_mae = val\n",
        "      best_mse = ret[0]\n",
        "      best_ci = ret[-1]\n",
        "      best_epoch = epoch+1\n",
        "      model_file_name = '1000yikes' + i + str(epoch) + '.model'\n",
        "      torch.save(model.state_dict(), model_file_name)\n",
        "      print('predicting for test data')\n",
        "      G,P = predicting(model, device, test_loader)\n",
        "      saved_p[epoch] = P\n",
        "      print('rmse improved at epoch ', best_epoch, '; best_mae,best_mse,best_ci:', best_mae,best_mse,best_ci) #,model_st)\n",
        "    else:\n",
        "      print('no improvement since epoch ', best_epoch, '; best_mae,best_mse,best_ci:', best_mae,best_mse,best_ci) #,model_st)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For Binary_Loss_Incorporated_Convolutional_Graph_Isomorphism_Network the following modifications were made to the code"
      ],
      "metadata": {
        "id": "JSLa8Uhp9AVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "#ReArranged train set so that the values further (at least .04) away from -6.93 are at the bottom of the set and thus only used in the validation set (this is to reduce throwing off loss due to small perterbations in the predicted value)\n",
        "df = pd.read_csv('ntrain.csv')\n",
        "\n",
        "fig = px.histogram(df, x='3CLPro_pocket1', nbins=7000, opacity=0.7)\n",
        "fig.update_layout(title='Distribution of 3CLPro_pocket1', xaxis_title='Values', yaxis_title='Frequency')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "3OTEnw_A9ZPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train function was modified to include variable bin_labels which was just the set of batch split binary encoded vectors for the train set in lue of these vectors being in the test bed\n",
        "def train(model, device, train_loader, optimizer, epoch, bin_labels):\n",
        "    print('Training on {} samples...'.format(len(train_loader.dataset)))\n",
        "    model.train()\n",
        "    c = 0\n",
        "    sig = nn.Sigmoid()\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        binar = np.asarray(bin_labels[c])\n",
        "        c = c + 1\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "\n",
        "\n",
        "        #The following postion of the train function was modified to include binary loss into our loss function\n",
        "        ################################################################################################\n",
        "        threshold = -6.93\n",
        "\n",
        "        binary_tensor = torch.zeros_like(output)\n",
        "        binary_tensor[output > threshold] = 1.0\n",
        "        a = binary_tensor\n",
        "        b = torch.tensor(binar, dtype=torch.float32)\n",
        "        b = b.view(a.shape)\n",
        "        b = b.to(device)\n",
        "        lb = torch.abs(torch.sum(torch.sub(a, b)))\n",
        "        loss = loss_fn(output, data.y.view(-1, 1).float().to(device)) + lb\n",
        "\n",
        "        ################################################################################################\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % LOG_INTERVAL == 0:\n",
        "            print('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n",
        "                                                                           batch_idx * len(data.x),\n",
        "                                                                           len(train_loader.dataset),\n",
        "                                                                           100. * batch_idx / len(train_loader),\n",
        "                                                                           loss.item()))"
      ],
      "metadata": {
        "id": "RJtuaw_C9ZlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The following code in the train-validation loop was changed so that the order was no longer shuffled, and the split ensured that the train set did not have affinity values close to defined threshold of -6.93\n",
        "\n",
        "train_size = int(0.8 * len(train_data))\n",
        "valid_size = len(train_data) - train_size\n",
        "\n",
        "valid_data = train_data[train_size:train_size + valid_size]\n",
        "train_data = train_data[:train_size]\n",
        "\n",
        "bat = list(df['encoded_array'])\n",
        "input_list = bat[:train_size]\n",
        "TRAIN_BATCH_SIZE = 512\n",
        "bin_labels = [input_list[i:i+TRAIN_BATCH_SIZE] for i in range(0, len(input_list), TRAIN_BATCH_SIZE)]\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=False)\n",
        "valid_loader = DataLoader(valid_data, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda:{}\".format(0)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = GINConvNet().to(device)\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "\n",
        "saved_p = {}\n",
        "mse_d = {}\n",
        "mae_d = {}\n",
        "ci_d = {}\n",
        "best_mse = 1000\n",
        "best_mae = 1000\n",
        "best_ci = 0\n",
        "best_epoch = -1\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  train(model, device, train_loader, optimizer, epoch+1, bin_labels)"
      ],
      "metadata": {
        "id": "vqoNjiEA9aFx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}